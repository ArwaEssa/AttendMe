{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "605a1e94",
   "metadata": {},
   "source": [
    "# MVP :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb86dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import glob\n",
    "from PIL import Image\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator , img_to_array, load_img\n",
    "from keras.applications.mobilenet import MobileNet, preprocess_input \n",
    "from keras.losses import categorical_crossentropy\n",
    "import splitfolders\n",
    "import shutil\n",
    "from keras.layers import Input, Lambda, Dense, Flatten\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a341c286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2,os\n",
    "import random\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras import layers\n",
    "import plotly.graph_objects as go\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16 \n",
    "from keras.applications.vgg19 import VGG19  \n",
    "from tensorflow.keras.applications import mobilenet_v2\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score,f1_score, confusion_matrix\n",
    "from keras.layers import Conv2D, Dense, MaxPooling2D, Activation, Dropout, Flatten,InputLayer\n",
    "from tensorflow .keras.preprocessing.image import img_to_array,load_img\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47da14cf",
   "metadata": {},
   "source": [
    "## Read data images :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35386cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5028567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "CATEGORIES = os.listdir(data_path)\n",
    "print(CATEGORIES)\n",
    "print(len(CATEGORIES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d1148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "IMG_SIZE=128\n",
    "def making_trian_dataset():\n",
    "    for category in CATEGORIES:\n",
    "        path = os.path.join(data_path, category) # path to our data\n",
    "        class_num = CATEGORIES.index(category)# classifcation index\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path,img))\n",
    "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE)) # resizing our imgz\n",
    "                training_data.append([new_array, class_num])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "making_trian_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549103f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a256ed",
   "metadata": {},
   "source": [
    "## Shuffle data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b52e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0863810e",
   "metadata": {},
   "source": [
    "## Data Labeled :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b521915e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X=[]\n",
    "y=[]\n",
    "for features, label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "X = np.array(X)#.reshape(-1, IMG_SIZE,IMG_SIZE,3)\n",
    "y = to_categorical(y, num_classes = len(CATEGORIES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75bd663",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb04ca6",
   "metadata": {},
   "source": [
    "## Data Splitting For Simple NN :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a9318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsample,nx,ny,ni=X.shape\n",
    "X1=X.reshape(nsample,nx*ny*ni)\n",
    "print(X1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b8d256",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X1_val_test, y1_train, y1_val_test = train_test_split(X1, y, test_size=.2, random_state=77)\n",
    "X1_val, X1_test, y1_val, y1_test = train_test_split(X1_val_test, y1_val_test, test_size=0.5, random_state=77)\n",
    "print(f\"\\nTraining data: {X1_train.shape},  labels: {y1_train.shape}\")\n",
    "print(f\"Validation data: {X1_val.shape},  labels: {y1_val.shape}\")\n",
    "print(f\"Testing data: {X1_test.shape},  labels: {y1_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a963e0bf",
   "metadata": {},
   "source": [
    "## Data Splitting :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a394ac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=.2, random_state=77)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=77)\n",
    "print(f\"\\nTraining data: {X_train.shape},  labels: {y_train.shape}\")\n",
    "print(f\"Validation data: {X_val.shape},  labels: {y_val.shape}\")\n",
    "print(f\"Testing data: {X_test.shape},  labels: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7634d91",
   "metadata": {},
   "source": [
    "## VGG19 Model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade96187",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = [128, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082279e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg19 = VGG19(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdcf8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't train existing weights\n",
    "for layer in model_vgg19.layers:\n",
    "  layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d4841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful for getting number of classes\n",
    "folders = glob('try/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200aa162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our layers - you can add more if you want\n",
    "x = Flatten()(model_vgg19.output)\n",
    "# x = Dense(1000, activation='relu')(x)\n",
    "prediction = Dense(len(folders), activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e19a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model object\n",
    "model_20_vgg19 = Model(inputs=model_vgg19.input, outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b82a53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the structure of the model\n",
    "model_20_vgg19.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b0d5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell the model what cost and optimization method to use\n",
    "\n",
    "model_20_vgg19.compile(\n",
    "    loss = 'sparse_categorical_crossentropy',\n",
    "    optimizer = 'adam', \n",
    "    metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8564e6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_20_vgg19.save_weights('weights_20_vgg19__11:30am.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2356273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "chechpoint = keras.callbacks.ModelCheckpoint(\n",
    "    'xception_v1_{epoch:02d}_{val_accuracy:.3f}.h5',\n",
    "    save_best_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26168334",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_V19 = model_20_vgg19.fit(X_train,y_train, \n",
    "                                validation_data=(X_val,y_val), \n",
    "                                batch_size=64,\n",
    "                                epochs=20,\n",
    "                                callbacks=[chechpoint]\n",
    "                                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2583a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "plt.plot(result_V19.history['loss'], label='train loss')\n",
    "plt.plot(result_V19.history['val_loss'], label='val loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('LossVal_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d097705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracies\n",
    "plt.plot(result_V19.history['accuracy'], label='train acc')\n",
    "plt.plot(result_V19.history['val_accuracy'], label='val acc')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('AccVal_acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5277a6dd",
   "metadata": {},
   "source": [
    "## load model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9883afea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "model_vgg19.save('save_model_20_vgg19__11:30am.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282a73d1",
   "metadata": {},
   "source": [
    "# Emotion detection :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fc6921",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dict= {'Angry': 0, 'Sad': 5, 'Neutral': 4, 'Disgust': 1, 'Surprise': 6, 'Fear': 2, 'Happy': 3}\n",
    "face_image  = cv2.imread(r\"/images/aidai/0_0_aidai_0014.jpg\")\n",
    "plt.imshow(face_image)\n",
    "# resizing the image\n",
    "face_image = cv2.resize(face_image, (48,48))\n",
    "face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)\n",
    "face_image = np.reshape(face_image, [1, face_image.shape[0], face_image.shape[1], 1])\n",
    "model = load_model(\"model_v6_23.hdf5\")\n",
    "predicted_class = np.argmax(model.predict(face_image))\n",
    "label_map = dict((v,k) for k,v in emotion_dict.items()) \n",
    "predicted_label = label_map[predicted_class]\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5872aba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
